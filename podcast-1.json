{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Scaling Multi-Modal Generative AI with Luke Zettlemoyer - #650", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI podcast. I am your host, Sam Charrington. And today I'm joined by Luke Zettelmoyer. Luke is a professor at the Paul G. Allen School of Computer Science and Engineering at the University of Washington and a research manager at Metta. Before we get into today's conversation, be sure to take a moment to head over to your listening platform of choice. And if you enjoy the show, please leave us a five-star rating and review. Luke, welcome to the podcast. Thanks, Sam. I'm really excited to be here. I'm super excited to have you on the show as well. We will have a lot to cover focusing on your work in multimodal generative AI, open source and open science, the effect of data on models and more. But before we do that, I'd love to have you share a little bit about your background and how you came to work in the field. Absolutely. So I've been at the University of Washington for about 13 years. So I've been faculty for a while. And before that, I got into AI through grad school and so forth. And then I've been at Metta for about over five years now, kind of looking at scaling things, looking at different applications of things, trying to do larger scale LLM type work. But really, you know, the interest kind of for me spans across like lots of different kind of use cases. These models are really fascinating. We don't really understand how they work or why they work. And they're also kind of locked away. And can we open them up? Can we give people access to them? Can we study them more to make them more usable? These kinds of things. And I don't know, I've always just been kind of fascinated about, like, what's the limit of what we can do with models? And that's really what drove me to the field. And it's kind of still what I'm doing now, if that makes sense. Nice. How has the recent popularity of large language models impacted your research agenda? It's really exciting. I think the resources involved to do the research changes. And so you have to kind of figure out what kind of work you're going to do. Like training one of these models is now a gigantic team and a supercomputing effort. And a few years ago, individual researchers could train state of the art models for things and everything has just really changed. But then also, as one of my students, Ari Holtzman often says, sometimes our whole field has shifted. Like now, whereas we used to be more of an engineering or algorithmic science kind of thing, we're almost now like a complex system science or natural science. Like we have these emergent things. We built them, so it's not exactly the same thing as studying the weather. But we still don't understand how they behave. We understand how the individual neurons interact. There's nothing surprising in the math, but the emergent behavior, we don't understand. That's really fascinating to me. That's like a whole new thing that didn't exist before. In what ways does your research focus in on trying to gain that understanding relative to some of the other topics that we'll be jumping into, like multimodal and other things? Do you have projects that are kind of probing into these models and trying to understand how they work? Yeah, absolutely. I mean, I think we've done a whole variety of different projects over the years. Some places where we've done a lot of analysis, for example, is in a multilingual setting. So the fact that you can, you know, that every model is multilingual now is kind of shocking. You can try to filter the data. So my student Tara Blevins had this really nice paper. You can try to filter the data, but classifiers have one percent error. So even if you try to train only on English, one percent of hundreds of billions of tokens is a lot of data and the models will be multilingual no matter what. And that's fascinating to me. So it's kind of hard. I haven't done a lot of work on like mechanistic understanding. I know there's great work at Entropic and other places on that. But even just like thinking about data's effect on how the model behaves and what the models can and can't do when you train them different ways and exactly what emerges, I think that's also a type of analysis, maybe more kind of input output style that I find really interesting. Yeah, you know, when I think about the topics that again, mentioned that we'll be talking about and that kind of are driving your current research, multimodal effect of data on models, open source, like kind of articulate how those all tie together. What's the unifying thread? Yeah, I mean, I'm sort of freewheeling. I kind of have a lot of threads and I enjoy that, like this sort of variety. But roughly speaking, I don't know that I really have one perfect thread. Some researchers are better at that than others. I just I love curiosity. I love going in lots of directions. But one thing I really feel really strongly, I do feel really strongly about like the effect of data. And so one interesting trend is like we're on this incredible scaling curve right now. Right. So models are getting bigger. We're spending more compute and actually parameter size probably isn't the right metric. Probably like amount of compute and amount of data is the right metric vaguely. But one cool thing is like as we scale, you have to scale both of those lock and stuff. Right. And is always going to have more compute and more data and probably more parameters too. But the interesting thing is to think about where these trends break as a researcher. And you want to like think out a little bit. So, for example, like we're going to run out of data someday. There's no question. Right. And especially if you're thinking about text only models, it's going to be sooner rather than later. Like you could try to guess how many trillions of tokens of text there are on the web. And, you know, Google and Bing have way more of that than anyone else. So there's like lots of extra caveats, but it's probably not much more than a order of magnitude larger than what we're training on right now. I mean, maybe somebody else could come up with another estimate, but we're going in order of magnitude every year roughly right now. So the runway probably isn't that long. So one cool thing with multimodal is kind of just that raw scaling trend. So if you're going to run out of text, you should be training on more interesting data. So that's kind of silly. It's kind of formulaic. Another aspect of it that I find really exciting is like the grounding problem. So the models learn a lot about how the world works, a lot about common sense from text only. But I think most researchers or at least a lot of researchers think that text only isn't going to get you there. And you need to have visual grounding and eventually maybe embodiment. But even as a step towards embodiment, maybe video, maybe audio, maybe other things into the model. And I think that's where most of the big model training projects are kind of going over time. I think you're going to see basically all the models move from being text only to be multimodal over the next year or two. And it's an exciting trend because there's a lot more signal, a lot more to learn from. And hopefully the models will be able to do really interesting new things with that extra data. Video and images come up frequently as a potential way to ground text and to make models richer. I'm curious, do you have a favorite example in research, like a favorite paper or something like that, that demonstrates this? And I guess I'm asking because it's always still to some degree a future. Like it's a direction that folks are going as opposed to we have demonstrated it to a degree, though. And I'm wondering what your favorite example of that is. I see. Yeah, I mean, I don't have deep insights that you don't have already here. But, you know, Dolly 3 just came out. The pictures are amazing and highly compositional. And they've really they've got some pretty deep understanding of some of the spatial stuff in the text. Right. And a lot of the compositionality of describing objects in the world and there's other ways of learning about compositional semantics and how things compose. Language and code gives you interesting windows into that, too. And that is kind of grounding if you ever actually execute that code. But I think the sort of the next token prediction in words is a very direct relationship between a piece of text and the next piece of text. But the text image is much less direct. And so the fact that these models can produce these beautiful images with these very compositional suggests that there's something really interesting going on there. You're learning things there that you wouldn't have learned from the text alone. One of our longer term goals, I think you were hinting at also in your question was like, could we ever say add images to a text only language model? And then it would be better at a text only task than if it had been trained without those images. That's kind of a holy grail, showing transfer from images back into the text domain. And I haven't seen any examples of things like that yet, but I think we will get there. And earlier this year, I think January this year, we had a scaling laws of multimodal competition paper where we looked at training LLMs. Armin was the first author on this. We looked at training LLMs on single modalities, pairs of modalities, and then a bunch of modalities all at once. And what we were trying to predict is when would bimodal training outperform unimodal training for given resource levels and so forth. And the scaling law is not perfect. We could talk about that if you want. But it makes predictions and it does predict that at some point, like training on images and text would be better than training on either alone if you can get to sufficient scale. And that's one of our research efforts right now is try to test that hypothesis and go bigger scale and see if we can get there. You hit on what I was hoping you would hit on and the kind of the sense I'm getting of kind of where we're trying to go and where we are. Can you elaborate on the paper you mentioned a little bit more? How was that? How was it set up? Yeah, absolutely. So we have a research agenda going on right now. A bunch of folks who had the scaling law paper and then more recently, we had a chameleon paper that kind of followed up on that. I think you mentioned at the beginning a little bit. And the mindset is that even though diffusion based models are kind of the rage for image generation right now, because we want to be able to put more and more modalities all into a single model, we're consciously stepping away from that. And instead, we're going back to the image sort of discretization tokenization approach. And so, you know, you use VQ GANs, you tokenize the sequences of tokens. And then basically, I'm an NLP person, right? So when I look at a continuous signal, I think I wish that was a sequence of tokens because I would really know what to do with it a lot better. And so basically, I'm just going to take any data I see and I'm going to tokenize it. And I'm going to turn into a sequence of tokens. And eventually, I'd like to go byte level. This tokenization has irrecoverable loss. But that's where we are right now. Eventually, we'll push that further. But anyways, now I can look at any data in the world and then I can just say, OK, that's a sequence of tokens. And I can apply all my LLM tricks to train models. So this has cool implications in terms of being able to train with any combination of text. And so let's just look at text and images, although people certainly don't know for audio and for video, not yet, but it will be soon. But if I look at any data, I can have any tokens in, any tokens out. So it's not text to image or image to text. It's just, you know, mixtures of text and image to mixtures of text and image. OK, that's kind of the story. And then the models work. And then you can do interesting things like train a bunch of different models, some on text only, some on image only, some on mixtures, and then look at their scaling properties as you scale the amount of data or you scale the number of parameters. It's a tradition kind of scaling loss sort of stuff. And you fit a power law and you see where we think there'll be crossing points. So where we think for a fixed amount of resource, you'd prefer to train bimodally instead of just training unimodally. And you would get a better model for both of the modalities, if that makes sense. So when you talk about seeing images as a sequence of tokens, how is that distinguished from how we would traditionally look at an image? When you say a token, is that a pixel vector or is it something else? Oh, yeah. Good question. So the way these tokenizers work is there's a discrete vocabulary. It's really the same thing as an NLP model. An NLP model, you have a vocabulary of 50,000 word pieces or maybe up to 100,000 words. In the image tokenizer, you're going to have a vocabulary of maybe 10,000 words or 20,000 words or even less, sometimes single thousand words. And what those words are is just less clear. So they're kind of gen-sims, like they're arbitrary symbols. It's not the word the in English. It's just some arbitrary symbol. And then what you've trained as a model that can take an image, map it to a sequence of, let's say, 500 or a thousand of those words. And then you throw away the image, you have another model that, given those image words, can map it back to the original image. And so you train the autoencoder that way, you get those two models. And then you can tokenize your data. So literally everything just becomes words. It's just some of the words in this special image vocabulary that we know how to turn back into images. And some of the words are in a BPE that we know how to turn back into. And it's actually tokenized even on the text side. If you know the BPE details, for some listeners that do, there's no vectors, there's no nothing. It's just discrete symbols. And then we know how to apply all of our really cool NLP techniques very, very broadly. And we get lots of really interesting functionality and things we can do. Like, for example, a single model that can take arbitrary mixtures of text and images and input and in the output, for example. Interesting. And so in this example, I think there's an LLM involved in this work here. Are you training on these tokenized images? Are you fine tuning on these tokenized images or something altogether different? Both. The traditional LLM training pipeline now is basically, like you just said, two steps, right? You've got your pre-training step, which is typically done at incredible scale. It's very, very expensive. And it's all next token prediction for all the documents you can steal off the web or whatever. Your second step, which is either you pay people to label data or you get existing data sets, you do some supervised fine tuning, which is generally a little bit cheaper, but the data is really secret sauce. And then you worry about safety and things like that. So with multimodal models, that same recipe hasn't always been followed. So maybe it is, maybe it's not. There aren't that many open papers on how the pre-training works, but it hasn't been explored as much. Typically, you would just kind of... I mean, there's definitely safety work and tuning, but it's not as established. So for example, one thing we can do, and this is what the Chameleon paper was a few months ago, is we just apply that same recipe. We do exactly the same steps. We curate as much data as we can. That's text or images or mixtures of the two. And then we pull together a bunch of label data sets, kind of like Flon style fine tuning. And we do those two stages, but we do it multimodal. And when you do your multitask fine tuning, some of those are text to image tasks, some of those are image to text tasks, some of those are other things. It doesn't matter. The Sango model can do all those different things. And so you can tune on them all and you can get a very general, very controlled model by just kind of copying the recipe from LLMs directly over to multimodal. And that's kind of a real advantage of our sort of tokenize everything worldview, is that anything that works for LLMs, we're expected to work here. Another example is we've done versions of it that are retrieval augmented. So I don't know how much you talked about retrieval augmentation in previous episodes and stuff. So we have that and it works. And Michi, a student at Stanford that was an intern with us last summer, led that effort. And it was, I think, a nice email paper this year. I can't remember which conference, but it's on archive. And you can do retrieval augmentation, not exactly the same, but similar to the Bing chat GPT is retrieval augmented or whatever. You go and pull in documents, you put them in the context, you read them. That all works. You can literally use the same code. It's just the documents can be multimodal. And so we showed that there's really cool long tail phenomenon. So for example, if you want to generate a picture of a particular architectural style, like an Armenian church, they have very specific like steeple styles. OK. And it's a long tail thing. It's definitely in the pre-training data that all the image generation models are trained on, but it's not common. But if you can retrieve an example of that and put that in the context, the model can look at it when it's doing its generation, it can actually get it right. It can produce a much, much better image that actually has the right architectural style, for example. Interesting. Is part of the way that you evaluated this model relative to the diffusion models? Do you see this as a demonstration of this interesting effect of tokenizing, or do you see this as a promising approach for image generation on its own merits? I think that it is a promising approach for image generation on its own merits. And I think that the field shifted to stable diffusion for lots of good reasons. So one of the reasons is efficiency. A lot of the not necessarily stable diffusion, but just diffusion based techniques in general. Just fusion models. Yeah. So one reason is efficiency, although we're finding you can train transformers much, much more efficiently and do more efficient inference with them. So I think that gap is going to go away over time. I mean, H100s have transformers on ships. So transformers are going to catch up in terms of efficiency. And then the other thing is that the tokenization, you have some irrecoverable loss. Like I said, like a thousand tokens just isn't enough to perfectly reconstruct an image. But if you look, for example, at Google's Party paper, which is a really beautiful paper, you know, that's a token based approach. I thought that was an equation. A picture is worth a thousand tokens. Exactly. I'm always not good at landing that joke, but that's one I should probably try for every time. But you know, Google's Party came out at the same time as I think ImageGen. I can't remember exactly the two, but one was image based and one was a diffusion based. And if you look at the images, they're both great. Both technologies do an amazing job of generating images. And most of the trade off is like efficiency and so forth. And we think we can solve the efficiency problem. So we're hoping to like reinvigorate the token based transformer based image generation and has all these extra advantages that you can do these very universal things. You can use the same models for everything. You don't have to have like a separate model for image generation than you have to have for text generation. We got to prove that and we got to show that that actually scales. That's kind of a current research effort for us. But we're pretty confident that it's going to get there. Interesting. Another paper that I wanted to dig into with you is the self alignment with instruction back translation paper. That's an interesting one. Can you talk a little bit about the motivation there? Yeah, Shan did a really, really nice job with that paper. It was really great. I think the, again, we talked about sort of in pre-training, there's the two steps. There's the really large scale next token prediction. And then there's the later fine tuning, which tends to be kind of smaller scale, just a little bit of data or in recent comparison, very, very tiny amounts of data. I think my personal prediction is that over time, these distinction is going to get kind of blurred. Okay, so there is lots of data on the web that looks like instruction data. That's why the model can zero shot stuff because it sort of saw it in its pre-training. And exactly what you put where and how you blend these things is, I think, is a really interesting area to explore. So this instruction back translation in a way was a first step towards that. And the idea was if you could get enough data that looks like instruction input out prepares, like a prompt you might show to a chat thing like chat GPT, if you can get enough of that data, you could train on an incredible scale and you could eventually, maybe you could get much better, more instructable models that can really do more things. And then the way you would get all that data is not obvious. It could be super expensive to pay annotators or do the other things. But we can borrow this trick from the machine translation literature. It's called back translation. And the cool insight here is that if you can get examples, if you're trying to learn a function from X to Y, whether it's a translation thing or whether it's an instruction thing, if you can get lots of good examples of high quality Ys, it's often easier to have a model write the X, which is the input anyways, and then use a real output Y that you gather. So you go find lots of really good things like essays you wish chat GPT could have written for you or other documents. And then all you have to do is kind of come up with a prompt that would actually yield that output and you've got a training example. And so you can get models to do that very easily. I think we did it a few shots with off the shelf model. It's not that hard to write the input. It's much, much easier than writing the output. And then you can make essentially unlimited amounts of data. And then you can study how much does it help to actually scale up that second stage relative to the first stage or how do you sort of do more interesting ways of blending them together. Now granted, one big use of LLMs is to create content like you might find on the web. What do you find at the instructions that are learned or this tuning approach more broadly? Is it generalizable to lots of different types of content or does it just create a model that's really good at creating web articles? I think that it does generalize. I don't know that we have a perfect answer for how much it generalizes. I do think of it as like unlocking abilities the model already has when you do this fine tuning. If you want to get a lot of different instructions stuff, what you want to do is kind of cover a lot of different domains. In some internal experiments we did at some point, we found various versions of Llama or whatever. It wasn't very good at poetry generation out of the box, just from the pre-training without the alignment. But then we found that if you fine tune it on like three prompts of like write a poem about this and then example poems, maybe slightly more than three, but a very small number, it would unlock poetry very generally. So it could do all kinds of other styles of poetry. You hadn't taught it. It kind of had that knowledge, but it was somehow not exposed to the prompting. But it didn't make it better at like writing cooking recipes, but it made it better at like poetry more generally. So it's like you get a certain amount of generalization beyond what you exactly show it, but you still want to get lots of examples of lots of different stuff across the whole range of different things you want it to do. And in fact, one of the examples from the paper was demonstrating that the model gets better at a math problem, kind of like a word problem. If you remember that example, it was something like, you know, John ran, it was like two track times and then it separately gave the length of the track and it asked the LLM to figure out his average pace. And I think without this approach, I think it was maybe raw llama. It was way off. And then with this approach, it was better. It's amazing that that works. Like why does this approach make the model better at that kind of reasoning problem? Yeah. So I think the reason is exactly the same thing as I was just saying for the poetry, poetry is like more of a style of text, whereas math maybe requires more reasoning. I would say the model has those abilities already, but they're kind of not exposed. They're not promptable. They're not too noble. They're not having been sort of tuned in a way that they know how to respond to the prompts yet, because maybe on the web it didn't see a prompt exactly like those word problems or at least not in the right setting. We have some of those examples in the instruction back transition data. And so that exposes not that exact word problem, but some sort of math things that give it the ability to kind of expose that information already has. That's my intuition. I don't think we have a super hard science to really definitively nail that and say that's absolutely what's happening. But that's my that's our best intuition right now. Yeah. I think that this is getting into the kind of kind of clearly getting into the we don't understand why these models work the way they do and kind of the whole emergent properties idea. Complex system science type angle. That's right. Yeah. The untuned model, untuned as in not using the approach demonstrated in this paper, it's not that that kind of the before model couldn't follow the formatting like it. The formatting was plausible. It's just that the numbers were wrong. And I guess the the formatting was better in the sense of like it broke down the problem a little bit better and it didn't get thrown off by kind of an easy but wrong approach in that example. But yeah, it's just really surprising that this kind of tuning produced that kind of result. Yeah, yeah, I think so. And I think it just see a few examples of that math being done right. If the numbers being threaded and it can often get it. Although I would say in general, all the models are still a little bit brutal on the math front, right? Like for the amount of resources and the amount of data they've seen, the fact that they still struggle with certain types of arithmetic is like a little bit a little bit surprising. So that's another thing I would love to analyze more and understand more over time. But I think that's just kind of where we are. And people do a lot of specialized tuning. We don't really know what goes into chat GPT before it gets released. But I would be shocked if there isn't specialized math tuning going on before it gets released. Yeah. And was that math oriented example chosen because you think there's some special relationship between the approach that was described in the paper and having models do math better or was it just an interesting example? I think it's more of an interesting example to show like the breadth of domains that it works for. It wasn't like specifically designed to work with math, for example. Okay. You talk about the creation of the seed data set. I think the seed was kind of manually curated, right? So what you want to do is sort of write examples that you think look like good input out of pairs to start things out from. Okay. And can you talk about kind of the scale of the seed versus the result that you kind of part of the point here is that you don't have to manually label, create a lot of instruction examples. I can't remember the final number. I think it was in the thousands. I'd have to go review in the paper exactly, but it's not a huge number. And I think in general, people have done a lot of alignment work. Thousands of examples is reasonable. It's not that expensive. Annotators know how to do it. You can even just do it yourself if you spend, you know, maybe not a day. But remember, you also don't necessarily have to write the full output, right? You can find the outputs, just write the prompts. So you only have to kind of write a sentence for each example, that kind of thing. I think it's a reasonable approach. I mean, I think that you could do a lot more research in making it more efficient and like using a smaller seed. But for me, the more interesting thing would be to like, try to scale it bigger, try to understand like what happens as you get more and more instruction data for your alignment stage and what types of data you actually need, like a more fine-grained analysis of how much you can generalize like we were talking about before, the different tradeoffs and so forth. In the context of the paper and more broadly, is there a difference between an instruction and a prompt? Like when you're coming up with these instruction examples, or you shake your head no, so that's the... You're coming up with the prompts that would generate the output as opposed to some meta instruction that when operated on a prompt would produce the output or something like that. No, it's very simple. It's just if I were to type this into an LLM, I would hope it would produce the output that's there. That's it's very simple. Given the method that you propose, you have this two-step approach, well, or three-step approach, but you kind of iterate and produce two candidate models. Would you expect that you can continue to improve the results by continuing to iterate like three and four and... It will saturate eventually. I don't know exactly. I don't expect that 10 iterations would really work, but I think future work get there. And especially if you like expand the scope as you iterate, I think it's really interesting to explore in the future, but I don't think we have a good answer yet. A lot of these iterative methods, the prior is that two, three iterations is what you need. You would dream that it would just take off and get better and better over time, but that's actually surprising and hard to achieve in practice. So alignment has come up multiple times in talking through these papers as a concept, and you've got a paper that focuses on alignment called Lima, Less is More for Alignment. Can you talk a little bit about that paper? Yeah, Chanting did this work. It's really great. Again, it has a practical aspect in the sense that we're trying to understand alignment, make systems better by aligning them, but also this really nice analysis science aspect of what's actually going on at this stage and how much data do you actually need and what kind of data. And so Chanting asked this really interesting question, how simple can you make it? Do you really need all the RLHF? Do you really need all the labeled data and so forth? And so she did an experiment of very carefully hand curating a small set of supervised data for alignment. I think it ended up being around a thousand examples and then just did really simple supervised fine tuning on that data with no extra algorithms or anything. And the model was kind of shockingly good, way better than you would expect. I mean, it certainly doesn't solve the alignment problem, but way better than you expect given so little signal. And so again, this gets back to some of those themes we were talking about before. But to me, that really suggests that most of what the model is doing was already there during pre-training and the alignment is just kind of exposing. That's the evidence I have, not perfect, but the evidence I have that you're just kind of exposing functionality that was there and it's not learning as much from the alignment as we think, at least at the scale that people are often doing at the smaller scale. What tasks did you evaluate on? I had to go look at the exact paper, but I think it was kind of the standard task that people look at. Okay. I forget the exact set. Yeah. The model that was created with the curated data set did, I forget if you said well or better, but can you provide any more color on the relative performance and what you saw there? Roughly speaking, it would be like if you had an unaligned model here and you had like a really good aligned model over here, you'd be sort of 80% of the way there on various sort of zero-shot prompting tasks or whatever. So you wouldn't get all the way. And if you got like really out of domain so far from the kinds of topics we covered in those thousand prompts, that would be more challenging for the model. But the more you're closer, the better. So for example, the poetry example I gave earlier, where you only had to put in a few poems, that was part of that work. It could do a lot of different poems more than just the two or three that we put in the thousand examples. And actually one interesting thing when we talk about, this is one of my pet peeves, so I'll just go off from the side rant, sorry. But we actually don't know how to evaluate these models very well. So if I gave you like chat GPT-1 and chat GPT-2, I'm sure OpenAI has their own better way of doing it. But in terms of public knowledge and good science and academia and open science and so forth, we actually don't have a good answer for how to tell you whether one is better than two. Okay, so people play with them, they interact with them, but we actually don't know how to do that. And I think that the fact that it's better on certain benchmarks, maybe you're doing MMOU, maybe you're doing whatever, is nice. But that is not a good proxy of how general that model is and how it's going to do when you deploy it and it interacts with millions of users or something. And the challenge of actually figuring out which model is better for really unrestricted prompting, I think that's a grand challenge for the field right now. That's super open and really unclear how to do it. So that's a little bit why I was also kind of hesitating when you're asking me about was it actually better, how much was it better. I actually don't think we know how to do that. What does that really mean? In a convincing way, yeah. I don't think we know how to do that. But it shouldn't stop us from working on the models, but we should just be realistic about what we're actually able to measure and practice. So people are careful about the claims we're making. Yeah, it reminds me a lot of a recent interview with James Oh of Stanford where they looked at, well, just the example that you gave, GPT-3, GPT-35, and compared their performance. But even in that conversation, that comparison is very specific. When you think about how do these things perform, you want to characterize them broadly. But they found that they perform worse on chain of thought reasoning, but they perform better on these things. Performance isn't uniformly improved, but one would think that OpenAI valued alignment in certain dimensions over performance on chain of thought reasoning in certain dimensions. And that's kind of the way the models were pushed. Yeah. Maybe they just didn't have those benchmarks in whatever their development cycle was. And maybe it's not always intentional. I am an OpenAI doing amazing work, and so probably they did. But I suspect you don't even have to make those trade-offs. But I think one interesting thing is that there's always going to be gaps, right? Because when you put a model in front of a user and allow it to arbitrarily prompt it, that's one of the big breakthroughs in the GPT. I guess it was three or maybe two, I'm not sure. But just the notion that you can really prompt for anything and you can produce any kind of natural language processing problem to a prompting problem in a language model. That was a really revolutionary idea. It seems obvious in hindsight, but everybody else missed it. But then once you make that shift in worldview, you realize that actually we have no idea how to evaluate that. Because literally you can do anything with the model. So how are you going to get a fixed set of K examples that test for anything? K is going to have to be really big and it's going to be really complicated. And then there's the fact that static evaluations are not a great proxy for long-form generation. They're not a great proxy for chat. There's just so much going on in these models that we don't really know how to evaluate, which I think is great. I think that's fascinating as a researcher, right? Lots to do. But you should also be realistic about where we are. Absolutely. You are also really excited about open source and open science as a way to advance the field. Talk about why that is so important to you. Yeah, I think that there's lots of reasons why open science is important. I think that sharing your ideas publicly puts them up to scrutiny and the work gets better. I think that having more people thinking about these problems is good and will make progress better as a field. And I think in general, giving people access to models that they can do cool things with also just creates lots of opportunities. Look at the startup ecosystem around Lama models, right? It's really beautiful and amazing. And it is true that, I don't want to discount the fact that the models can be used for harm. That is true, but there are great safety controls and we're always getting better at that. And I also want to remember that the models can be used for incredible good too. They can do really amazing things. And that when you lock off one, you lock off the other two. So I think it's really cool to see the amazing things people can do with the models when you release them. I think we learn more as a field and we move faster. And I think it's just really like a democratizing effect that everybody can participate and we're not kind of locking people out. And all those are very, very exciting to me. Going back to a point you made very early in the conversation is, has it become somewhat of a necessity for the vast majority of researchers in the sense of the resources required to train these large models from the scratch is so huge. If there wasn't an open source and open science aspect of the field, it would be very difficult for many researchers to be involved. Do you agree with that? I do agree with that. I do agree with that. I think also, it's important to have full access to all the parameters of the model, like really be able to download the model. So for example, if you look at API access, which a bunch of companies starting with OpenAI have done, those models often change behind the scenes. Think about you want to do science and you've run experiments with a model and then somebody pushed an update to the model and now you can't recreate your paper. These kinds of things. It's just really, really bad for reproducibility, which is a hallmark of doing good science. It's one of our goals. We need to be able to study things, measure them and reproduce it. And if we can't do that, it's really, really bad for the field. Thinking about the areas that you're working on, talk to us a little bit about the future that you see between multimodal, focusing on data, open source. Where's your research agenda headed and what do you see down the line? I think models are going to get much, much better. They're going to get even more expensive for a while. And then the interesting trends like we talked about earlier, when you run out of data, what do you do? Maybe then you can start doing really cool work on getting as much out of existing data as possible. Right now, we're getting low hanging fruit because we can always just keep scaling. Eventually, we're going to maybe have some hope to do more interesting stuff and go deeper with it. But overall, I feel like we're just in this time of incredible change. And the models are going to keep getting better and better. But good luck predicting exactly where they're going. If you look back even 12 months, you couldn't predict where we'd be today, right? So I don't know that I have super concrete predictions about exactly what the breakthroughs are going to be. But I think data is going to be a component. Eventually, things are going to hopefully get more efficient. The way we're doing the scaling right now, we're all copying one recipe from OpenAI. A little bit of adapting. But largely, we're just copying one recipe. And I think we need new recipes. We need to think about efficiency. We need to think about how wasteful it is to do some of this stuff. And I think there'll be some big breakthroughs in this area for sure. And then hopefully, that will democratize the models even more, make them more usable by people, make us understand them better. So I think a lot of that will happen. But I couldn't tell you, within 18 months, we'll have an XYZ model that will do this other thing. I'm not that good. I'm not that great of a predictor. Yeah. Now, granted that we've spent the last 45 minutes or so talking about some of the ways that you think these recipes will change, are there other ideas that come to mind in terms of promising ideas or approaches that will impact the way we train these models? Yeah. I think one that I'm very excited about is just like sparsity and conditional compute. The notion that you're not just densely connecting everything to everything across the layers in the network, but a lot more like how the brain works, that there's very sparse interconnect and so forth. I don't know that much about how the brain works, but it's still a great inspiration in terms of its power usage and things like that. And I think there'll be some big breakthroughs there. And I think the rumor has it that a lot of the big companies have stopped publishing what they're doing. Right. And so that's kind of sad. We're losing access to a lot of information about the latest round of models. Tech reports are getting less and less details about the methods every time they come out. But rumor has it that the big labs have been investing a lot in conditional compute, mixture of experts, sparsity. Because in the end, you can only take this done scaling that we're on so far. You can buy 100,000 H100s. That's amazing. I'm sure somebody's going to do it. But then eventually you've got to do something more creative because even getting those to all talk to each other, there's like laws of physics limitations that are coming up and you've got to think about sparsity and other ways of doing things. I think eventually it keeps scaling. So given the limitation that you just mentioned in terms of kind of scaling around dense networks and the previous limitation you mentioned around kind of running out of textual data Do you feel like progress will slow relative to kind of this burst of progress we recently had? I mean, at some point, yes, but I wouldn't bet on when. I guess I mean near term. Do you think there's a near term eminent slowing before we come up with the next thing that kind of creates the next elbow on the curve? Or do you think we still have runway on the things we're doing now that will give us time to figure out the next thing? I think we still have runway at least a few years and eventually we're going to get into video. The amount of data and video as compared to text, there's a lot of data and video. And we could be looking at podcasts, for example. We could be listening to them and look at video here is not that exciting. But imagine you could train on all of YouTube. What could you learn about civilization from that? You could learn a lot, right? And so I don't think we're necessarily going to run out of data soon. I think we've got a long runway, but maybe text only is not going to be the main focus after a while. That's more my feeling. And that there's going to be huge sort of need to come up with better algorithms or maybe just better versions of GPUs, I don't know, to actually make sense of all that data. So the scaling trend, I think, is alive for a while, but just a little different looking than it currently is. Awesome. Well, Luke, thanks so much for taking the time to chat about what you're up to. It's very exciting work. Yeah, thanks for having me. It's super fun. Thank you. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimbleai.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "Title: Exploring Multimodal Generative AI and Open Science\n\nSpeakers: Sam Charrington (host) and Luke Zettelmoyer (guest)\n\nKey Insights:\n- Luke Zettelmoyer, a professor at the University of Washington, discusses his work in multimodal generative AI, open source and open science, and the impact of data on models.\n- He is interested in understanding the behavior and workings of large language models (LLMs) and making them more usable and accessible.\n- The recent popularity of LLMs has changed the resources required for research, with training now being a large-scale team effort.\n- LLMs are more like complex systems, and their emergent behavior is not fully understood.\n- Zettelmoyer's research focuses on studying the effect of data on model behavior, especially in multilingual contexts and multimodal tasks.\n- He believes that as models scale, there will be a need for more interesting and diverse data, such as images and videos, to train models effectively.\n- Tokenization, where images are represented as sequences of tokens, opens up possibilities for training multimodal models and improving efficiency.\n- Zettelmoyer highlights the importance of open source and open science in advancing the field, as it enables collaboration, reproducibility, and a democratizing effect.\n- Future directions include exploring sparsity and conditional compute, as well as scaling models to handle video data.\n\nOverall, Zettelmoyer's work focuses on understanding and improving large language models through the use of multimodal data and open science principles."}